{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #lin. Alg.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as matimg\n",
    "\n",
    "#importing keras modules\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Input, Lambda, Add, Subtract\n",
    "from keras.layers import Conv2D, Softmax, Reshape\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D, ReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.models import load_model, Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "#Python Image Library for image processing\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "#Path of parasite and uninfected folders\n",
    "path=\"E:MachineLearning/DVHacks/images-and-features/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sahil\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sahil\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sahil\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sahil\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sahil\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clear GPU Memory of Keras Models\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n",
      "(2001, 40)\n",
      "(2001, 512)\n"
     ]
    }
   ],
   "source": [
    "f = None\n",
    "z = None\n",
    "\n",
    "for i in os.listdir(path):\n",
    "    if(i[0:5]=='pred_'):\n",
    "        if f is None:\n",
    "            f = np.load(path+i).reshape(1,40)\n",
    "            z = np.load(path+'z_vector'+i[5:9]+'.npy').reshape(1,512)\n",
    "            print(z.shape)\n",
    "        else:\n",
    "            f = np.concatenate((f,np.load(path+i).reshape(1,40)))\n",
    "            z = np.concatenate((z,np.load(path+'z_vector'+i[5:9]+'.npy').reshape(1,512)))\n",
    "print(f.shape)\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sahil\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               20992     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "=================================================================\n",
      "Total params: 23,040\n",
      "Trainable params: 22,016\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (40,)\n",
    "\n",
    "def MyModel():\n",
    "    x = Input(shape=input_shape)\n",
    "    #y = Dense(128, activation = 'relu')(x)\n",
    "    o = Dense(512)(x)\n",
    "    o = BatchNormalization()(o)\n",
    "    #o = Lambda(lambda o:o*8)(o)\n",
    "\n",
    "    return Model(x, o)\n",
    "\n",
    "model = MyModel()\n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "batch_size = 32\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False, clipvalue=0.5)\n",
    "model.compile(loss=keras.losses.mean_absolute_error,\n",
    "              optimizer=Adam(0.002),\n",
    "              metrics=['accuracy', 'mae'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 401 samples\n",
      "Epoch 1/250\n",
      "1600/1600 [==============================] - 1s 598us/step - loss: 0.9397 - acc: 0.0038 - mean_absolute_error: 0.9397 - val_loss: 0.9078 - val_acc: 0.0100 - val_mean_absolute_error: 0.9078\n",
      "Epoch 2/250\n",
      "1600/1600 [==============================] - 1s 328us/step - loss: 0.8433 - acc: 0.0000e+00 - mean_absolute_error: 0.8433 - val_loss: 0.8588 - val_acc: 0.0000e+00 - val_mean_absolute_error: 0.8588\n",
      "Epoch 3/250\n",
      "1600/1600 [==============================] - 1s 318us/step - loss: 0.8184 - acc: 0.0031 - mean_absolute_error: 0.8184 - val_loss: 0.8339 - val_acc: 0.0050 - val_mean_absolute_error: 0.8339\n",
      "Epoch 4/250\n",
      "1600/1600 [==============================] - 1s 317us/step - loss: 0.8076 - acc: 0.0038 - mean_absolute_error: 0.8076 - val_loss: 0.8193 - val_acc: 0.0050 - val_mean_absolute_error: 0.8193\n",
      "Epoch 5/250\n",
      "1600/1600 [==============================] - 0s 309us/step - loss: 0.8010 - acc: 0.0031 - mean_absolute_error: 0.8010 - val_loss: 0.8112 - val_acc: 0.0050 - val_mean_absolute_error: 0.8112\n",
      "Epoch 6/250\n",
      "1600/1600 [==============================] - 1s 329us/step - loss: 0.7971 - acc: 0.0031 - mean_absolute_error: 0.7971 - val_loss: 0.8076 - val_acc: 0.0050 - val_mean_absolute_error: 0.8076\n",
      "Epoch 7/250\n",
      "1600/1600 [==============================] - 1s 328us/step - loss: 0.7951 - acc: 0.0038 - mean_absolute_error: 0.7951 - val_loss: 0.8046 - val_acc: 0.0050 - val_mean_absolute_error: 0.8046\n",
      "Epoch 8/250\n",
      "1600/1600 [==============================] - 1s 317us/step - loss: 0.7940 - acc: 0.0038 - mean_absolute_error: 0.7940 - val_loss: 0.8027 - val_acc: 0.0075 - val_mean_absolute_error: 0.8027\n",
      "Epoch 9/250\n",
      "1600/1600 [==============================] - 1s 326us/step - loss: 0.7931 - acc: 0.0013 - mean_absolute_error: 0.7931 - val_loss: 0.8019 - val_acc: 0.0050 - val_mean_absolute_error: 0.8019\n",
      "Epoch 10/250\n",
      "1600/1600 [==============================] - 1s 314us/step - loss: 0.7927 - acc: 0.0013 - mean_absolute_error: 0.7927 - val_loss: 0.8013 - val_acc: 0.0075 - val_mean_absolute_error: 0.8013\n",
      "Epoch 11/250\n",
      "1600/1600 [==============================] - 1s 327us/step - loss: 0.7926 - acc: 0.0038 - mean_absolute_error: 0.7926 - val_loss: 0.8008 - val_acc: 0.0075 - val_mean_absolute_error: 0.8008\n",
      "Epoch 12/250\n",
      "1600/1600 [==============================] - 1s 316us/step - loss: 0.7923 - acc: 0.0044 - mean_absolute_error: 0.7923 - val_loss: 0.8007 - val_acc: 0.0025 - val_mean_absolute_error: 0.8007\n",
      "Epoch 13/250\n",
      "1600/1600 [==============================] - 1s 319us/step - loss: 0.7921 - acc: 0.0025 - mean_absolute_error: 0.7921 - val_loss: 0.8007 - val_acc: 0.0000e+00 - val_mean_absolute_error: 0.8007\n",
      "Epoch 14/250\n",
      "1600/1600 [==============================] - 1s 321us/step - loss: 0.7919 - acc: 0.0044 - mean_absolute_error: 0.7919 - val_loss: 0.8010 - val_acc: 0.0050 - val_mean_absolute_error: 0.8010\n",
      "Epoch 15/250\n",
      "1600/1600 [==============================] - 1s 324us/step - loss: 0.7918 - acc: 0.0031 - mean_absolute_error: 0.7918 - val_loss: 0.8009 - val_acc: 0.0050 - val_mean_absolute_error: 0.8009\n",
      "Epoch 16/250\n",
      "1600/1600 [==============================] - 1s 326us/step - loss: 0.7917 - acc: 0.0031 - mean_absolute_error: 0.7917 - val_loss: 0.8010 - val_acc: 0.0050 - val_mean_absolute_error: 0.8010\n",
      "Epoch 17/250\n",
      "1600/1600 [==============================] - 1s 329us/step - loss: 0.7915 - acc: 0.0031 - mean_absolute_error: 0.7915 - val_loss: 0.8009 - val_acc: 0.0000e+00 - val_mean_absolute_error: 0.8009\n",
      "Epoch 18/250\n",
      "1600/1600 [==============================] - 1s 317us/step - loss: 0.7916 - acc: 0.0056 - mean_absolute_error: 0.7916 - val_loss: 0.8011 - val_acc: 0.0075 - val_mean_absolute_error: 0.8011\n",
      "Epoch 19/250\n",
      "1600/1600 [==============================] - 1s 331us/step - loss: 0.7915 - acc: 0.0031 - mean_absolute_error: 0.7915 - val_loss: 0.8012 - val_acc: 0.0025 - val_mean_absolute_error: 0.8012\n",
      "Epoch 20/250\n",
      "1600/1600 [==============================] - 1s 333us/step - loss: 0.7913 - acc: 0.0031 - mean_absolute_error: 0.7913 - val_loss: 0.8014 - val_acc: 0.0050 - val_mean_absolute_error: 0.8014\n",
      "Epoch 21/250\n",
      "1600/1600 [==============================] - 1s 327us/step - loss: 0.7913 - acc: 0.0013 - mean_absolute_error: 0.7913 - val_loss: 0.8014 - val_acc: 0.0075 - val_mean_absolute_error: 0.8014\n",
      "Epoch 22/250\n",
      "1600/1600 [==============================] - 1s 314us/step - loss: 0.7912 - acc: 0.0031 - mean_absolute_error: 0.7912 - val_loss: 0.8013 - val_acc: 0.0000e+00 - val_mean_absolute_error: 0.8013\n",
      "Epoch 23/250\n",
      "1600/1600 [==============================] - 1s 330us/step - loss: 0.7911 - acc: 0.0031 - mean_absolute_error: 0.7911 - val_loss: 0.8011 - val_acc: 0.0050 - val_mean_absolute_error: 0.8011\n",
      "Epoch 24/250\n",
      "1600/1600 [==============================] - 1s 321us/step - loss: 0.7910 - acc: 0.0050 - mean_absolute_error: 0.7910 - val_loss: 0.8011 - val_acc: 0.0100 - val_mean_absolute_error: 0.8011\n",
      "Epoch 25/250\n",
      "1600/1600 [==============================] - 1s 319us/step - loss: 0.7911 - acc: 0.0031 - mean_absolute_error: 0.7911 - val_loss: 0.8013 - val_acc: 0.0050 - val_mean_absolute_error: 0.8013\n",
      "Epoch 26/250\n",
      "1600/1600 [==============================] - 1s 314us/step - loss: 0.7910 - acc: 0.0063 - mean_absolute_error: 0.7910 - val_loss: 0.8012 - val_acc: 0.0025 - val_mean_absolute_error: 0.8012\n",
      "Epoch 27/250\n",
      "1600/1600 [==============================] - 1s 323us/step - loss: 0.7910 - acc: 0.0050 - mean_absolute_error: 0.7910 - val_loss: 0.8011 - val_acc: 0.0050 - val_mean_absolute_error: 0.8011\n",
      "Epoch 28/250\n",
      "1600/1600 [==============================] - 1s 322us/step - loss: 0.7908 - acc: 0.0019 - mean_absolute_error: 0.7908 - val_loss: 0.8011 - val_acc: 0.0025 - val_mean_absolute_error: 0.8011\n",
      "Epoch 29/250\n",
      "1600/1600 [==============================] - 1s 331us/step - loss: 0.7907 - acc: 0.0025 - mean_absolute_error: 0.7907 - val_loss: 0.8014 - val_acc: 0.0025 - val_mean_absolute_error: 0.8014\n",
      "Epoch 30/250\n",
      "1600/1600 [==============================] - 1s 329us/step - loss: 0.7907 - acc: 0.0044 - mean_absolute_error: 0.7907 - val_loss: 0.8013 - val_acc: 0.0050 - val_mean_absolute_error: 0.8013\n",
      "Epoch 31/250\n",
      "1600/1600 [==============================] - 1s 327us/step - loss: 0.7905 - acc: 0.0038 - mean_absolute_error: 0.7905 - val_loss: 0.8018 - val_acc: 0.0100 - val_mean_absolute_error: 0.8018\n",
      "Epoch 32/250\n",
      "1600/1600 [==============================] - 1s 335us/step - loss: 0.7906 - acc: 0.0038 - mean_absolute_error: 0.7906 - val_loss: 0.8016 - val_acc: 0.0050 - val_mean_absolute_error: 0.8016\n",
      "Epoch 33/250\n",
      "1600/1600 [==============================] - 1s 322us/step - loss: 0.7905 - acc: 0.0044 - mean_absolute_error: 0.7905 - val_loss: 0.8011 - val_acc: 0.0025 - val_mean_absolute_error: 0.8011\n",
      "Epoch 34/250\n",
      "1600/1600 [==============================] - 1s 329us/step - loss: 0.7906 - acc: 0.0013 - mean_absolute_error: 0.7906 - val_loss: 0.8014 - val_acc: 0.0025 - val_mean_absolute_error: 0.8014\n",
      "Epoch 35/250\n",
      "1600/1600 [==============================] - 1s 336us/step - loss: 0.7904 - acc: 0.0019 - mean_absolute_error: 0.7904 - val_loss: 0.8017 - val_acc: 0.0050 - val_mean_absolute_error: 0.8017\n",
      "Epoch 36/250\n",
      "1600/1600 [==============================] - 1s 331us/step - loss: 0.7904 - acc: 0.0038 - mean_absolute_error: 0.7904 - val_loss: 0.8014 - val_acc: 0.0025 - val_mean_absolute_error: 0.8014\n",
      "Epoch 37/250\n",
      "1600/1600 [==============================] - 1s 337us/step - loss: 0.7903 - acc: 0.0038 - mean_absolute_error: 0.7903 - val_loss: 0.8017 - val_acc: 0.0050 - val_mean_absolute_error: 0.8017\n",
      "Epoch 38/250\n",
      "1600/1600 [==============================] - 1s 340us/step - loss: 0.7902 - acc: 0.0056 - mean_absolute_error: 0.7902 - val_loss: 0.8013 - val_acc: 0.0050 - val_mean_absolute_error: 0.8013\n",
      "Epoch 39/250\n",
      "1600/1600 [==============================] - 1s 340us/step - loss: 0.7902 - acc: 0.0013 - mean_absolute_error: 0.7902 - val_loss: 0.8016 - val_acc: 0.0050 - val_mean_absolute_error: 0.8016\n",
      "Epoch 40/250\n",
      "1600/1600 [==============================] - 1s 341us/step - loss: 0.7903 - acc: 0.0056 - mean_absolute_error: 0.7903 - val_loss: 0.8017 - val_acc: 0.0025 - val_mean_absolute_error: 0.8017\n",
      "Epoch 41/250\n",
      "1600/1600 [==============================] - 1s 330us/step - loss: 0.7900 - acc: 0.0038 - mean_absolute_error: 0.7900 - val_loss: 0.8013 - val_acc: 0.0075 - val_mean_absolute_error: 0.8013\n",
      "Epoch 42/250\n",
      "1600/1600 [==============================] - 1s 339us/step - loss: 0.7901 - acc: 0.0031 - mean_absolute_error: 0.7901 - val_loss: 0.8017 - val_acc: 0.0050 - val_mean_absolute_error: 0.8017\n",
      "Epoch 43/250\n",
      "1600/1600 [==============================] - 1s 330us/step - loss: 0.7900 - acc: 0.0044 - mean_absolute_error: 0.7900 - val_loss: 0.8019 - val_acc: 0.0025 - val_mean_absolute_error: 0.8019\n",
      "Epoch 44/250\n",
      " 820/1600 [==============>...............] - ETA: 0s - loss: 0.7891 - acc: 0.0024 - mean_absolute_error: 0.7891"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e2a887eb032a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf14\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf14\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_num = 250\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(f,z,test_size = .2, random_state = 42)\n",
    "\n",
    "model_log = model.fit(x_train, y_train, epochs=epoch_num, batch_size=10, verbose=1, validation_data=(x_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot accuracy vs Epochs\n",
    "plt.plot(model_log.history['acc'])\n",
    "plt.plot(model_log.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('human_feature_to_z.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/401 [==============================] - 0s 50us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8021456734497946, 0.007481296758104738, 0.8021456734497946]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.77025194e-02  8.81230790e-03  1.34246302e-02  1.51890255e-02\n",
      "  -2.02293698e-04  5.77887305e-03 -7.85315179e-04 -5.54116272e-03\n",
      "   1.09529287e-02  2.50836570e-03  1.47998907e-02 -1.64470181e-03\n",
      "  -1.75492489e-02  2.84912326e-03  1.43585603e-02 -1.00506419e-02\n",
      "  -5.81229434e-03  4.07101246e-03  1.24338182e-02 -1.44329540e-02\n",
      "   1.15719380e-02  1.61654862e-02  1.17975130e-02 -7.17732321e-03\n",
      "  -9.41442543e-03  5.05424458e-03 -8.02659435e-04  1.74884595e-03\n",
      "   1.02892387e-02  4.20410728e-03  1.74535819e-02 -1.62508143e-03\n",
      "  -1.44824856e-02  2.23812606e-03 -1.05011632e-02  6.56406086e-03\n",
      "   9.60633454e-03  7.38554801e-04 -7.20864866e-03 -1.75497566e-02\n",
      "   1.25218238e-02 -1.01538487e-02 -6.42756887e-03  2.44320631e-03\n",
      "  -6.21524512e-04 -2.43219961e-03 -2.20604349e-02  1.33952821e-02\n",
      "   5.23276072e-03 -1.05287642e-02 -2.71281698e-03  2.21995456e-03\n",
      "   1.61205467e-02  1.67534785e-03  6.41732786e-03  1.07069003e-02\n",
      "  -1.33594092e-04  1.72802447e-02  1.49041564e-04  8.79511492e-03\n",
      "   2.60460226e-02  7.20975091e-03 -4.06328630e-04 -2.18394284e-03\n",
      "  -7.71286047e-03  4.55535127e-03  2.80936806e-03 -6.95966196e-03\n",
      "  -6.17323236e-03 -4.05577524e-03  1.50324749e-02  1.17886365e-02\n",
      "   2.40279797e-03 -4.76424755e-03  1.74811851e-02  9.79633455e-03\n",
      "  -1.38006139e-02  1.85757470e-02 -4.50150228e-03  3.85708116e-03\n",
      "  -4.88315859e-03 -1.37673135e-03  1.51749747e-02 -8.01957209e-03\n",
      "   3.44285210e-03 -9.75033190e-03  4.26767147e-03 -3.69494382e-03\n",
      "   2.64435494e-03  1.32826732e-02  5.64383404e-03 -1.05553774e-04\n",
      "   3.78322205e-03 -1.99218842e-02  1.07210267e-02 -1.60113601e-03\n",
      "  -5.99121513e-03  2.00953854e-02  5.57739150e-03  1.44400740e-03\n",
      "  -6.25503821e-03 -1.78412654e-03  1.65291505e-03 -4.73611983e-03\n",
      "   1.69635447e-02 -1.49333744e-02  1.21168382e-02  3.81852475e-03\n",
      "   1.66630941e-02 -1.42976501e-02 -1.10826218e-02 -1.34537672e-03\n",
      "   1.51549702e-02 -9.28899787e-04 -2.44057215e-02 -1.78669105e-02\n",
      "   4.91109986e-03  1.02949902e-02  1.34397335e-02  1.73363868e-02\n",
      "   2.35678906e-02 -1.01938163e-02  1.25190462e-03  8.79389133e-03\n",
      "   1.37472180e-02  2.35819568e-03  1.08855421e-02 -9.01249195e-04\n",
      "  -8.80532075e-03 -3.67605701e-03  3.54052891e-03  2.35843236e-03\n",
      "  -2.43768884e-03 -5.13493417e-03  2.06139419e-03  2.26861172e-03\n",
      "  -5.54447004e-03 -3.24109955e-03 -3.46975739e-03  2.10736748e-03\n",
      "  -1.48195637e-02 -4.93360936e-03  8.29309217e-03  6.50211371e-03\n",
      "   1.25456399e-02 -1.35178129e-02 -7.26124873e-03  2.57125891e-04\n",
      "   6.90812359e-03 -6.66483692e-03  4.06207264e-03 -2.83068310e-03\n",
      "   1.22022695e-02 -4.49329194e-03  5.82410721e-03 -4.37279185e-03\n",
      "  -1.90959105e-03 -4.75850115e-03 -1.26484697e-03 -5.21910038e-03\n",
      "  -9.11197440e-04  6.17977490e-03 -3.24694843e-03  2.16188241e-02\n",
      "  -8.41404400e-03  1.39394338e-02 -1.31346713e-02 -2.85606899e-03\n",
      "  -6.75945261e-03  1.64479619e-02  5.09844309e-03  1.60728952e-03\n",
      "  -8.50516340e-04 -2.56029203e-02  3.23164522e-05  3.40110559e-03\n",
      "   3.69088678e-03 -1.68180688e-02 -1.13033925e-02  1.56195784e-02\n",
      "   1.56921439e-02 -9.76776486e-03  1.70561451e-02  2.53773734e-02\n",
      "  -1.38223498e-02 -6.14311044e-04  1.76009409e-02  3.49785808e-03\n",
      "   9.99725836e-03 -1.67434233e-02  7.78637876e-03 -7.39286260e-03\n",
      "  -1.98798936e-03  1.76847631e-02 -1.27331628e-02  1.10342554e-03\n",
      "  -1.46546579e-02  5.28887211e-03  1.28883195e-03  8.61349135e-03\n",
      "   1.55310251e-02 -4.12403015e-03 -8.08366371e-04 -1.87117522e-03\n",
      "  -3.82010941e-03 -5.41345748e-03  4.52198696e-03  8.85530132e-03\n",
      "   9.41317951e-03 -2.48917371e-02 -6.77588973e-03 -3.42936493e-03\n",
      "  -1.07479518e-02 -4.72002457e-03  2.27124095e-03  6.63803450e-04\n",
      "   3.86735255e-03  1.21131148e-03  1.03795245e-03 -5.19606776e-04\n",
      "   1.32037376e-03  1.22348920e-03  1.48227273e-02 -1.63017825e-02\n",
      "  -1.41563346e-02  1.11219579e-02  7.21105968e-03 -1.17649500e-02\n",
      "   4.11336664e-03 -9.65214057e-03 -1.13736490e-03 -1.41885377e-02\n",
      "   1.67856929e-02  5.65669235e-03 -2.93636587e-03  1.57952163e-02\n",
      "  -2.11757394e-03  1.01861033e-02 -1.19203508e-02  1.29301626e-04\n",
      "  -1.21144118e-04 -1.07234817e-03  7.66164110e-04  1.36617935e-03\n",
      "  -1.29967410e-02  3.05956947e-03  8.51154793e-03 -1.27973618e-02\n",
      "  -4.46210945e-03 -2.39824751e-02 -5.54941009e-03 -1.06735248e-02\n",
      "   2.60887899e-03 -9.99723707e-04  4.55862328e-05 -2.88656186e-04\n",
      "   8.26134313e-03  1.31302768e-03 -9.60495820e-03 -7.09652269e-03\n",
      "  -1.17423395e-02  6.47522740e-03 -3.80332159e-03  9.55312495e-03\n",
      "   3.56747446e-03  2.63807044e-02  4.08180939e-03  5.83639282e-03\n",
      "   3.17625641e-03 -1.22060685e-02  4.20250463e-04 -1.51365373e-02\n",
      "  -2.76475631e-03 -1.18184172e-02 -3.48387515e-03 -3.09815303e-03\n",
      "   2.82774611e-02  9.04436890e-03  1.57600404e-03  1.04804218e-02\n",
      "   1.39219948e-02 -1.21788283e-02  3.65426635e-03  1.02256915e-02\n",
      "   4.99186488e-03 -1.20294820e-03  1.08588286e-02  7.28369960e-03\n",
      "  -7.35836161e-03  1.08444518e-02  1.09319742e-02 -6.34816508e-03\n",
      "   3.15268639e-03  6.18477288e-03 -4.02581466e-03 -7.90444078e-04\n",
      "   6.42020784e-03  1.36710985e-02  1.54319059e-02  1.15376495e-02\n",
      "  -9.12720744e-03 -2.02668257e-03 -1.69152615e-02  7.23367107e-03\n",
      "   4.35818379e-03 -2.78484752e-03 -3.33539228e-03  1.98693020e-02\n",
      "  -7.94812307e-03  1.15841250e-03  5.87205180e-03  7.83987564e-03\n",
      "  -4.10875743e-03 -2.80036735e-03  4.85853711e-03  1.31145698e-02\n",
      "  -3.86614851e-03  3.03252185e-03  1.90225336e-03  1.00637417e-02\n",
      "  -2.70627716e-03 -4.18697218e-03 -3.91445318e-03 -3.27054355e-03\n",
      "   8.94498307e-03  5.66510899e-05  6.34774574e-03 -1.43427743e-02\n",
      "   3.34674064e-03 -6.31121992e-03  1.36026243e-03 -6.76823896e-03\n",
      "   1.28548616e-03  1.65118542e-02  1.97417617e-02 -6.90864859e-03\n",
      "  -2.30676241e-03 -1.40226406e-03  3.90630203e-03 -2.23983012e-02\n",
      "  -1.20152351e-02  1.48961219e-02 -8.64798213e-04  3.75143191e-04\n",
      "   2.80983768e-03  3.10953500e-03 -4.37353357e-03 -1.21726962e-02\n",
      "  -9.12877154e-03 -6.27065218e-03  1.24197906e-02 -7.15345127e-04\n",
      "  -2.44052716e-03 -8.57713928e-03  8.55512255e-03 -6.20941160e-03\n",
      "   1.43729794e-02  5.39372743e-03  9.43499818e-03  5.77364160e-03\n",
      "  -5.29147224e-03 -1.33118091e-02 -1.07434904e-02 -1.75804401e-02\n",
      "  -2.17649537e-04 -4.96313860e-03  1.40090231e-02  5.57093247e-03\n",
      "  -1.94585317e-02 -3.54813124e-04 -9.51072471e-03 -1.11086981e-02\n",
      "   2.46031996e-02  1.21458073e-02 -8.38805014e-03  6.55756018e-04\n",
      "  -7.01768003e-03  3.25917043e-03 -6.57571349e-03 -1.93375601e-02\n",
      "   4.89338429e-04 -1.13468502e-02  9.28508933e-03  2.35179609e-04\n",
      "   5.82956782e-03 -1.47545711e-03  2.96737141e-03  1.57978564e-02\n",
      "  -1.12950171e-02  2.71464499e-03 -3.68268568e-03 -1.30784324e-02\n",
      "   1.16030474e-02  5.71922725e-03  3.24357664e-03  1.09862728e-02\n",
      "   1.20068914e-02 -5.16188970e-03  6.87894156e-03 -1.24148477e-02\n",
      "   1.37050068e-02  1.58000949e-02 -1.40938259e-02 -6.02811213e-03\n",
      "  -1.49370592e-02 -3.43744364e-03  2.94525350e-03 -7.40306007e-03\n",
      "  -8.33699230e-03 -5.72345945e-03 -9.14728590e-03 -2.80698115e-03\n",
      "  -1.46863055e-03 -2.07245105e-02  1.69556085e-02 -6.75931327e-03\n",
      "   1.19840014e-04 -2.35253727e-02  7.69425096e-03 -1.22053718e-03\n",
      "   5.17053984e-03  1.39019264e-04 -1.40250868e-02  2.66561692e-03\n",
      "   2.61440435e-03 -1.26850318e-02  4.54225404e-03 -1.32213633e-02\n",
      "   4.88930575e-03 -2.24773149e-03 -1.63969902e-02 -7.99757008e-03\n",
      "   1.04932625e-02 -9.50678222e-03 -4.65252757e-03 -6.69496729e-03\n",
      "  -1.01496001e-02 -9.18129926e-03 -6.28107293e-04 -1.23090209e-03\n",
      "  -1.08713043e-03 -5.58962840e-03  7.87000813e-03  9.19426476e-03\n",
      "  -4.27231921e-03  4.49838042e-03 -1.01928261e-02 -8.91896508e-03\n",
      "  -1.33616920e-02  7.01445936e-03  8.46183581e-03  4.05479867e-03\n",
      "  -1.26428074e-02 -7.09879690e-03  1.39657443e-02 -5.58206494e-03\n",
      "  -3.92407016e-03 -3.22635054e-03  4.53381553e-03 -1.05181836e-02\n",
      "   6.83685002e-03  1.14337769e-02  2.00132057e-03 -8.78203709e-04\n",
      "  -1.18445266e-02  1.41269698e-02  4.22089414e-03 -1.13844922e-02\n",
      "   4.34740378e-03 -3.67524523e-03  1.43025795e-02  6.34799260e-03\n",
      "   8.42696703e-03 -9.92129057e-03 -5.11466187e-03 -8.14005741e-04\n",
      "  -8.10702610e-03 -1.96130561e-02 -2.84717999e-03  6.06518249e-03\n",
      "   1.41173703e-02  1.64075626e-02  6.98402192e-03  2.98071019e-03\n",
      "   6.01945422e-03  1.24745091e-02 -9.25358886e-04  4.41632382e-03\n",
      "  -3.01612389e-04  2.07853426e-03  7.06554943e-03 -3.85987018e-03\n",
      "  -2.09887728e-02 -8.72703531e-03  1.76509803e-03 -1.11504356e-02\n",
      "  -1.37105010e-03  1.17521732e-02 -2.70644582e-03  1.11908777e-02\n",
      "   1.58848413e-04 -1.03334229e-02 -1.35978061e-03  1.66009338e-02\n",
      "  -1.19338250e-02 -1.87336290e-04 -2.69043383e-03 -1.56178909e-02\n",
      "   3.07870379e-03 -7.73009386e-03  5.46067135e-03 -3.88443423e-04]]\n",
      "(1, 512)\n",
      "[6.63552024e-02 9.33644798e-01 4.14259837e-02 3.95443695e-01\n",
      " 1.63594185e-03 3.64442908e-03 9.34577149e-02 5.34745074e-01\n",
      " 2.97719210e-02 1.21360922e-03 4.78972069e-01 1.58586453e-03\n",
      " 6.49987976e-01 5.70076377e-04 8.93384620e-04 9.81399911e-01\n",
      " 6.43354036e-03 4.83353355e-03 2.03517511e-03 9.77423339e-01\n",
      " 3.87635286e-02 2.93161040e-02 5.43890428e-02 5.88412685e-01\n",
      " 4.32884692e-01 2.43091305e-01 4.67073836e-01 6.79609992e-03\n",
      " 3.40527926e-02 7.06727219e-01 3.50797163e-01 7.90377381e-01\n",
      " 0.00000000e+00 8.43922307e-01 8.93384620e-04 3.70843257e-01\n",
      " 1.47282419e-01 1.48498279e-01 1.47454568e-01 0.00000000e+00]\n",
      "[ 3.57760051e-01 -1.22672443e+00  8.55184559e-01  7.54530677e-01\n",
      " -9.05441958e-01 -2.97265802e-01 -1.31008556e+00 -4.90627518e-01\n",
      " -4.46856113e-01  2.96491424e-01  3.19606999e-01 -5.33719964e-01\n",
      "  3.25786334e-01  1.08703122e+00  2.16979157e+00 -2.78809138e-01\n",
      "  8.54007366e-01  6.92812008e-01  2.24366480e-02 -2.43488640e-01\n",
      "  1.46501714e-01 -1.48114647e-01  7.58387752e-01  6.29880157e-01\n",
      " -3.21364699e+00 -4.67707906e-01 -9.49844922e-02  1.26779077e-02\n",
      " -4.65398061e-01 -3.95295854e-01  8.59816660e-01 -2.44162866e-01\n",
      "  1.25254410e+00  1.00649770e+00 -3.18953988e-01 -9.84290132e-01\n",
      " -8.71148514e-02  5.56038682e-02 -1.01259891e-01  2.08958663e-01\n",
      "  1.20123905e+00  6.42104736e-01 -1.33950759e+00 -6.12210895e-01\n",
      " -9.15555100e-02 -4.61652076e-01  4.65934997e-02  4.89480795e-01\n",
      "  3.47899008e-01  5.96929390e-01 -5.31906336e-01 -1.54886610e+00\n",
      "  6.49373142e-01  1.85113220e-01 -1.38991958e-01  7.80360576e-01\n",
      "  5.49434994e-01 -1.76967618e-01  8.06240302e-01  9.00597400e-01\n",
      " -8.60053453e-01 -2.25859927e+00 -1.55709809e+00 -1.33151676e+00\n",
      " -1.01004329e+00 -1.28082653e+00 -8.17966325e-01  1.16154579e+00\n",
      " -1.98318919e-01 -2.87461331e+00 -6.13061314e-01 -3.71036973e-01\n",
      " -9.95271667e-01 -6.90598902e-01  3.47790734e-01 -7.73721020e-01\n",
      "  1.52461008e+00 -8.94379275e-01 -1.32697058e-01 -1.46591182e+00\n",
      " -5.64763282e-02 -1.52840130e+00  9.38601136e-01 -9.75323375e-01\n",
      "  1.07235159e+00 -9.25361718e-01  5.77272520e-01 -1.56184472e+00\n",
      " -7.69816929e-01  6.21647601e-01  3.54573644e-01 -1.41963833e+00\n",
      "  1.72245885e-01 -5.78118166e-02 -6.91348301e-01 -3.30735566e-01\n",
      "  1.60200508e-01 -1.67255296e+00 -2.17167849e+00 -7.96502570e-01\n",
      "  1.06414193e+00  3.74277810e-01 -8.12691885e-01 -1.13241540e+00\n",
      "  2.30640277e+00 -7.66946903e-01 -1.69710884e+00 -3.01553585e-01\n",
      "  3.80438741e-01  1.32737363e-01  7.91392857e-01  5.45043045e-02\n",
      "  1.17719958e+00 -9.26006465e-01 -4.11382109e-01  4.35183381e-01\n",
      " -1.17393475e-01  1.32382481e+00 -9.08935269e-01  1.47115370e+00\n",
      " -2.66265779e-01  1.91085100e-01  1.26902084e+00 -2.47370737e-01\n",
      "  1.05660478e+00 -3.24865745e-01  1.83963668e+00  1.64216163e-01\n",
      " -8.41708567e-01  1.16768413e+00 -1.72620201e+00 -7.42043147e-01\n",
      " -4.89521980e-01  1.45562500e+00 -1.86024347e+00  1.56951820e+00\n",
      "  6.66842647e-01 -4.14735870e-01  3.69919813e-01  7.69519569e-01\n",
      " -5.54553365e-02  9.15328617e-01  2.84598137e-01  5.32480724e-01\n",
      " -1.41712884e-01 -8.91918778e-01 -2.12794255e-01  5.91855626e-01\n",
      "  1.33395514e+00 -2.30781398e-01  9.30983285e-02 -7.63179919e-01\n",
      " -1.24948704e+00  2.52887431e-01 -9.91564275e-01  1.54675067e-01\n",
      " -1.00792143e+00 -3.19453329e-02 -1.21207807e-01 -4.04804961e-01\n",
      " -1.07080595e+00  1.21269014e+00 -1.41088508e-01 -8.06633883e-01\n",
      " -1.75182441e-01 -5.66931985e-01 -8.30985136e-01  1.14862040e+00\n",
      " -1.00950497e+00 -1.88643856e+00 -7.42207234e-01 -1.04363585e+00\n",
      " -2.13441436e-01  1.27459078e+00  5.84031879e-03  1.09668621e+00\n",
      "  1.66404871e-01 -4.57906833e-01  3.42323863e-01 -1.06510514e+00\n",
      " -3.02216317e-01 -1.21095134e-01 -1.45450050e+00 -1.13111382e+00\n",
      "  3.75114081e-01  2.80744830e-01 -2.12819297e+00  1.28993609e+00\n",
      " -8.47509158e-01  1.84230175e+00  1.62086496e-01  1.47409216e+00\n",
      " -5.88112885e-01  1.15700739e+00 -6.87427353e-01 -5.75594640e-02\n",
      " -1.42920311e+00 -2.39007776e-01  1.81572842e+00 -2.47702394e-01\n",
      " -6.96058166e-01  3.30923654e-01 -6.08455588e-02 -1.54277320e-01\n",
      "  1.63988140e+00 -1.53538475e+00 -7.23687880e-01  1.19285284e+00\n",
      " -1.95721984e-01 -3.77371004e-01  5.18602238e-01  3.80817180e-01\n",
      " -1.76597223e-01 -1.88530605e-01  3.14553983e-01  7.99340690e-01\n",
      " -7.20399317e-01  8.39999227e-02 -4.51600042e-01  1.79245643e+00\n",
      "  4.06609001e-02 -8.78160131e-01 -2.39584147e-01  5.51852043e-01\n",
      " -3.04603156e-01 -2.33662378e-01 -8.98949341e-01 -3.69286485e-01\n",
      "  9.44333335e-01 -2.66931017e-01  5.17626586e-02 -8.88253717e-01\n",
      "  1.14751027e+00 -7.12920474e-01 -8.21555770e-02 -3.21123173e-01\n",
      " -1.51512790e+00  5.82539717e-01  1.74856170e+00 -7.33141925e-01\n",
      "  1.04464320e+00  9.14351258e-01  6.82096563e-01  1.75687023e+00\n",
      "  1.52928325e+00 -5.35811677e-02 -1.02976214e+00 -3.20227297e-01\n",
      " -1.23932261e-01 -2.14748305e+00 -1.14271452e+00  2.56493604e-01\n",
      "  3.59502621e-02  1.44530201e+00  6.83527962e-02 -1.56002029e+00\n",
      " -7.01314308e-01  2.35008878e+00 -1.32838299e+00  2.02613450e-01\n",
      "  9.43561840e-02 -4.55505788e-03 -3.33892671e-01 -1.07096151e-01\n",
      " -1.91063994e-01  2.61078656e-01  5.43997831e-01 -1.35537554e+00\n",
      " -6.94290177e-01  6.61977376e-01  1.14877297e+00  1.58763086e+00\n",
      " -4.94996585e-01  5.96900565e-01 -4.96110656e-01 -2.27053936e-01\n",
      "  1.30502512e+00  9.58294944e-01 -3.29704368e-01 -1.55049534e-01\n",
      " -1.34286646e+00 -4.40196127e-01 -7.12602654e-01 -7.72108652e-02\n",
      "  2.81536943e-01  1.37445190e+00 -5.32509729e-01 -3.97870554e-01\n",
      " -1.04604101e+00  9.32388146e-01 -9.69460573e-01  1.65965765e+00\n",
      " -2.83219683e-01 -1.47969886e+00  2.94424953e-01 -5.09268452e-01\n",
      "  1.66445850e+00 -1.36535135e+00  1.18128431e-02 -8.06186524e-02\n",
      " -1.00245057e+00  1.41849577e+00 -3.27956310e-01  9.71990738e-01\n",
      " -4.21197438e-01  9.87951066e-01  8.85380695e-01  1.18035512e-02\n",
      "  9.37261224e-02 -1.03015885e+00 -2.76818519e-01  1.64024085e+00\n",
      " -1.13661701e+00 -5.42411246e-02  3.13067763e-01 -8.11780535e-01\n",
      "  7.93781900e-01  1.06248142e-01 -9.85879443e-02  3.77904797e-01\n",
      "  8.22723213e-01  1.02824716e+00 -2.18076096e-01  4.49055849e-01\n",
      " -1.10322190e-01 -5.32433841e-01 -1.32262544e+00 -2.50592798e-02\n",
      " -5.75604251e-02  1.50167697e+00 -1.60122709e+00  5.15125367e-01\n",
      " -3.56616929e-01 -1.18649869e-01 -2.67326974e-01  8.71377487e-01\n",
      "  3.88293985e-01 -4.91705521e-01 -2.23099851e+00 -4.80497091e-01\n",
      "  9.04785663e-02  1.02250364e+00  4.26857411e-01 -2.30675820e-02\n",
      "  2.83520438e-01  5.05851346e-01 -1.25695978e+00 -2.76613949e+00\n",
      " -1.46725856e+00 -4.05830390e-01 -3.64448381e-02 -1.35800347e+00\n",
      " -7.58933506e-01  1.00773446e+00 -4.30144427e-01 -1.92540874e-01\n",
      "  1.79313724e+00 -2.42830542e-01  1.20433852e+00 -1.65304522e+00\n",
      "  8.13731108e-02 -1.32571944e+00  1.04225806e-01 -1.00986940e+00\n",
      "  7.52996272e-02  6.02878296e-01  7.46504841e-02  1.45316588e-03\n",
      " -2.59165309e-01 -7.58718756e-01  1.79449541e-01 -4.69669549e-01\n",
      " -1.70073987e-01  1.01496296e+00  1.85129444e+00 -1.00536432e+00\n",
      " -1.19655943e+00  7.87652595e-01 -1.07389950e+00  2.00658248e+00\n",
      " -6.05056842e-01 -1.00262040e+00  1.87235529e-01 -5.29335250e-01\n",
      "  2.53796523e-01 -6.02358710e-01 -8.33478816e-02  9.04791182e-01\n",
      "  2.31980319e-01 -5.98507473e-01  1.12941060e+00 -6.38432664e-01\n",
      "  2.42718774e-01 -1.09362257e+00 -6.22749784e-01 -3.62151050e-01\n",
      " -1.39734019e+00  8.23229616e-01  3.18627823e-01 -6.58614317e-01\n",
      "  3.76373336e-01 -9.87861557e-01  2.38828869e-01  2.10068210e-01\n",
      " -4.54149342e-01  1.09204185e+00  4.10004152e-02  6.32322028e-01\n",
      " -2.10638138e+00  8.01047336e-02 -1.51772985e+00  8.91838133e-01\n",
      " -8.04179181e-01 -6.69508766e-01 -4.68005354e-01  4.27792095e-01\n",
      " -9.67697542e-01  8.45939095e-01 -1.36523092e+00 -9.30871262e-01\n",
      " -8.77418503e-01 -2.64394050e-01  4.10570961e-01 -1.05419756e+00\n",
      " -7.68893421e-01 -1.48172234e+00  2.41833246e-01 -6.17965517e-01\n",
      " -5.13655472e-01 -1.05969024e-01  2.54442651e+00  1.39757836e+00\n",
      "  1.03499165e+00  2.76368353e+00  8.68486633e-01  1.05077870e-01\n",
      " -1.35096074e+00 -5.23123307e-01  4.86048334e-01 -1.12189699e+00\n",
      "  6.01765352e-01 -2.85298970e-01  1.19033321e-01 -1.41801807e-01\n",
      "  4.45755637e-01 -1.82970726e+00  1.50493015e+00 -1.73336788e+00\n",
      "  9.47244162e-01 -1.08735407e+00 -1.28452724e+00 -2.87697600e-01\n",
      " -2.28789079e+00  5.42095877e-01 -1.44204221e+00  1.08741872e-01\n",
      "  1.08832838e+00 -6.44664390e-01 -2.02538243e+00  8.06175665e-01\n",
      "  6.84879131e-02 -1.18404820e-01 -2.26896272e+00 -1.27679240e-01\n",
      "  1.32867319e+00  6.22966197e-01 -1.04886120e+00 -1.58698186e+00\n",
      " -4.37794791e-01 -1.14354372e+00 -1.05738804e+00 -1.70007117e+00\n",
      "  4.37106963e-02 -1.41340914e+00 -1.34233310e-01  1.80187767e+00\n",
      " -8.41418846e-01 -2.82142436e-01 -3.10475485e-01  1.07959389e+00\n",
      "  1.30048045e+00  1.82905042e+00 -7.95435282e-01  1.83689779e-01\n",
      "  2.76184526e-01  8.34877571e-02  5.29935847e-02  1.32046838e+00\n",
      " -9.92500008e-01 -3.67320650e-01 -1.91574034e+00  1.10265354e-01\n",
      "  3.09754759e-01 -5.16823739e-01 -5.90988997e-01 -1.08457328e+00\n",
      " -1.42000638e+00 -5.92297606e-01 -5.56455883e-01 -1.57930627e+00\n",
      "  7.72609711e-01 -1.00734678e-01 -1.10450583e+00 -7.46684138e-01\n",
      "  1.48787529e+00 -6.12758754e-01  1.21975161e-01 -1.26884720e-01\n",
      " -8.52751684e-01  1.82622247e+00  8.40069864e-02 -1.50580686e+00]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_train)\n",
    "z_fin = np.zeros((1,512))\n",
    "for i in range (1600):\n",
    "    ztmp = pred[i] * x_train[i][2]\n",
    "    #print(x_train[i][0])\n",
    "    z_fin = np.add(z_fin,ztmp)\n",
    "z_fin = z_fin/1600\n",
    "print(z_fin)\n",
    "print(z_fin.shape)\n",
    "np.save('z_test.npy',model.predict(x_train)[0].reshape(1,512))\n",
    "np.save('stylegan2/asian_vec.npy', z_fin)\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
